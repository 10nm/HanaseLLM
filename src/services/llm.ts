import { GoogleGenAI } from '@google/genai';
import axios from 'axios';
import type { Config, ConversationHistory, LLMResponse, Result } from '../types.js';

/**
 * Google Gemini LLMã‚µãƒ¼ãƒ“ã‚¹
 */

/**
 * ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
 */
const createSystemPrompt = (): string => {
  const now = new Date();
  const formattedDate = `${now.getFullYear()}å¹´${now.getMonth() + 1}æœˆ${now.getDate()}æ—¥`;
  const formattedTime = `${now.getHours()}æ™‚${now.getMinutes()}åˆ†`;

  return `ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦é©åˆ‡ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚`;
};

/**
 * Gemini APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
 */
export const createLLMClient = (config: Config) => {
  if (config.llmProvider === 'local') {
    return null;
  }
  return new GoogleGenAI({ apiKey: config.geminiApiKey });
};

/**
 * ä¼šè©±å±¥æ­´ã‚’Gemini APIã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
 */
const formatHistory = (history: ConversationHistory) => {
  return history.messages.map((msg) => ({
    role: msg.role,
    parts: [{ text: msg.parts }],
  }));
};

/**
 * ãƒ­ãƒ¼ã‚«ãƒ«LLMç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
 */
const createLocalPrompt = (
  systemPrompt: string,
  history: ConversationHistory,
  userMessage: string
): string => {
  let prompt = `${systemPrompt}\n\n`;

  for (const msg of history.messages) {
    const role = msg.role === 'user' ? 'User' : 'Model';
    prompt += `${role}: ${msg.parts}\n`;
  }

  prompt += `User: ${userMessage}\nModel:`;
  return prompt;
};

/**
 * LLMã§å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹
 */
export const generateResponse = async (
  client: GoogleGenAI | null,
  userMessage: string,
  history: ConversationHistory,
  config: Config
): Promise<Result<LLMResponse, Error>> => {
  const startTime = Date.now();

  try {
    const systemPrompt = createSystemPrompt();

    // ãƒ­ãƒ¼ã‚«ãƒ«LLMã®å ´åˆ
    if (config.llmProvider === 'local') {
      console.log('ğŸ¤– Using Local LLM Provider');
      const prompt = createLocalPrompt(systemPrompt, history, userMessage);

      const response = await axios.post(
        `${config.localLlmUrl}/generate`,
        {
          prompt,
          max_new_tokens: config.maxTokens,
          temperature: 0.7,
          top_p: 0.9,
        },
        { timeout: 60000 } // é•·ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
      );

      const text = response.data.response;

      if (!text) {
        return {
          success: false,
          error: new Error('Empty response from Local LLM'),
        };
      }

      const processingTime = Date.now() - startTime;

      return {
        success: true,
        value: {
          text,
          processingTime,
        },
      };
    }

    // Gemini APIã®å ´åˆ
    if (!client) {
      return {
        success: false,
        error: new Error('Gemini client not initialized'),
      };
    }

    const formattedHistory = formatHistory(history);

    // ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
    const chat = client.chats.create({
      model: config.modelName,
      config: {
        maxOutputTokens: config.maxTokens,
        systemInstruction: {
          parts: [{ text: systemPrompt }],
        },
      },
      history: formattedHistory,
    });

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
    const result = await chat.sendMessage({ message: userMessage });
    const text = result.text ?? '';

    if (!text) {
      return {
        success: false,
        error: new Error('Empty response from LLM'),
      };
    }

    const processingTime = Date.now() - startTime;

    return {
      success: true,
      value: {
        text,
        processingTime,
      },
    };
  } catch (error) {
    return {
      success: false,
      error: error instanceof Error ? error : new Error(String(error)),
    };
  }
};

/**
 * LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹
 */
export const initLLMService = (config: Config) => {
  const model = createLLMClient(config);

  return {
    generate: (userMessage: string, history: ConversationHistory) =>
      generateResponse(model, userMessage, history, config),
  };
};
