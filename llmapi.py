import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

# --- 1. è¨­å®š ---
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã‚‹
print("--- Initializing settings ---")
base_model_id = "google/gemma-3-270m-it"
adapter_path = "/home/nm/gemma/gemma-3-270m-it-open2ch-lora"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# --- 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿ ---
# ã“ã‚Œã‚‚èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘å®Ÿè¡Œ
print("--- 1. Loading base model and tokenizer ---")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
tokenizer.pad_token = tokenizer.eos_token
print("--- Base model and tokenizer loaded ---")

import os

# # --- 3. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨ ---
# print("--- 2. Applying LoRA adapter ---")
# if os.path.exists(adapter_path):
#     try:
#         model = PeftModel.from_pretrained(base_model, adapter_path)
#         print("--- LoRA adapter applied successfully ---")
#     except Exception as e:
#         print(f"--- Warning: Failed to load LoRA adapter: {e} ---")
#         print("--- Continuing with base model only ---")
#         model = base_model
# else:
#     print(f"--- Warning: Adapter path '{adapter_path}' not found ---")
#     print("--- Continuing with base model only ---")
#     model = base_model
# print("\nğŸ‰ Model initialization complete. Starting API server. ğŸ‰")

# --- 4. FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®šç¾© ---
app = FastAPI()

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å®šç¾©
class PromptRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.9

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å®šç¾©
class GenerationResponse(BaseModel):
    response: str

# --- 5. APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ ---
@app.post("/generate", response_model=GenerationResponse)
async def generate(request: PromptRequest):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’ç”Ÿæˆã—ã¦è¿”ã™ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    # ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆå½¢å¼ã®å¯¾è©±ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    chat = [
        {"role": "user", "content": request.prompt},
    ]

    # ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    prompt_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

    # å…¥åŠ›ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    inputs = tokenizer(prompt_text, return_tensors="pt", add_special_tokens=False).to(device)

    # å¿œç­”ã‚’ç”Ÿæˆ
    outputs = base_model.generate(
        **inputs,
        max_new_tokens=request.max_new_tokens,
        do_sample=True,
        temperature=request.temperature,
        top_p=request.top_p,
    )

    # ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
    response_text = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)

    # ç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’JSONã§è¿”ã™
    return GenerationResponse(response=response_text)

# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã«ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
